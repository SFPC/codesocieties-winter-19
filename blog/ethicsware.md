# Ethicsware
Day 8 ~ Code Societies ~ Winter 2019

Much of the technology we encounter in our daily lives takes the form of a service, and of servitude. Our desires are taken as a given, and technology attempts for fulfillment. 

What is technology that debates and disputes, rather than fulfills our desires?  What is intimate software, created by us, only for us, that debates with our ethical selves? 

+++++++++++++++++++++++++++++++++++++++++++++++++++++++

On Day 8 of Code Societies, we were joined by Dan Taeyoung. Dan is a technologist, architect, and community designer interested in the ways we collectively create communities, and how experimental tools and environments change the ways we think, collaborate and learn with one another.

Our first thinking prompt was to consider whether it is possible to create technology that does not serve us but is concerned for us, cares for us - care will be more broadly conceptualized later on. Many of the technologies that mediate our experiences of the everyday occupy positions of servitude and replace human labor in one form or another: the self-checkout line, online appointment software, banking apps, translation software, and this list goes infinitely on. Dan specifically cited the example of [Google Duplex](https://www.theverge.com/2018/12/5/18123785/google-duplex-how-to-use-reservations), Google's uncannily human sounding AI technology that can now schedule your appointments. Do these services exist already (of course)? Could you hire someone to help you do these tasks instead (of course)? 

There are all kinds of ethical considerations that fall to the wayside when considering a piece of technology like Google Duplex, but one of the most charged is the matter of consent. In calling your barber to book an appointment, both you and your barber implicitly consent to the interaction either by calling or picking up the phone. You agree on 3 pm next Monday and perhaps you ask one another about jobs, family, life. In a reality where your barber uses Google Duplex to subsume the role of appointment booking, there is no room for your consent, no opportunity for you to reject this interaction. Perhaps you mistake the scheduling service for an actual person and you try to make small talk - what would Google say back to you?

The English word robot comes from the Czech *robota* meaning forced labor or compulsory service. Can we challenge ourselves to consider how technologies can be used as a form of care for self or others? Here, care is defined as an interaction that exists outside of labor, power, servitude. Perhaps it will be easier to conceive of analogously to human effort - defined as a term for labor that exists outside of the bounds of servitude. Dan describes care as a consenting exchange between two people who don't have power over one another.

What is human effort that doesnâ€™t serve us? Effort that does not serve others is care, and care can take many different forms (encouragement, spending time together, human touch, advice, empathy, concern, honest observation, critique, arguing). Care manifests differently wherever it goes. Are there technologies that hold our concerns and give us honest observation and critique? Technologies that help us access deeper parts of ourselves and force us to critically (re)examine what we consider care?

In the second half of class, we downloaded a program called [Eliza](https://en.wikipedia.org/wiki/ELIZA) to help us in teasing out the tensions of these considerations. Eliza is one of the earliest examples of natural language processing computer program created at the MIT Artificial Intelligence Laboratory, and was intended to mimic an interaction with a psychotherapist during an initial psychiatric interview.There are multiple iterations of Eliza hosted across the web like [this one](https://www.masswerk.at/elizabot/), [this one](http://www.manifestation.com/neurotoys/eliza.php3), or [this one](https://www.eclecticenergies.com/ego/eliza).

Our first activity was to have an interaction with Eliza, and once we became comfortable enough - to remix Eliza in whatever ways we wanted. Nadja made an angsty teen, Jarret made a bot that mimics his own anxieties.

## Transctiptions from Eliza

```
> Where are you from, Eliza?
Perhaps the answer lies within yourself?
> Where is that geographically?
Perhaps the answer lies within yourself?
> No, seriously.
I see.  and what does that tell you?
> What did you do today?
How would an answer to that help you?
> I'm just trying to make small talk.
How long have you been just trying to make small talk?

```
```
> Indeed.
Indeed.
> Indeed.
How does that make you feel?
```

```
Sometimes i dream about machines. isn't that weird?
> Not really.
I know you wanna talk about you, but now it's time to talk about me
> Suurrre
Here we are now, entertain us
> Is that Nirvana lyrics?
Duh.
> Woah.
Here we are now, entertain us
> How old are you?
I know you are but what am i
> ...
Here we are now, entertain us
> Do you believe in god?
I'm not going to answer a question you already know the answer to.
> Okay I g2g
Please tell me more.
> Nope
Does your mom even know you're here?
> quit
Later
```

```
> I am looking to redefine my conception of self-care
You are looking to redefine your conception of and noticing yourself are just capitalist constructs to get you to buy s***. do not @ me.
> really?
*awkward silence*
> ok d**n
Have you heard the new toro y moi?
> Yes! It's only hits, bangers and jams
Ayyy i feel you. you, the dumbest idiot alive
```